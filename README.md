# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #5 выполнил:
- Казанцев Михаил Максимович
- РИ - 210914
Отметка о выполнении заданий:

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 40 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;


Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код конфигурации агента. Результаты обучения.
- Задание 2.
- Подробное описание графиков в TensorBoard.
- Выводы.

## Цель работы
Интегрировать экономическую систему в проект Unity и подобрать её параметры с помощью ML-Agent'а

## Задание 1
Ход работы:
- Для начала, посмотрим как хорошо сможет обучится система с изначальной конфигурацией:

![config0](images/config0.png?raw=true)

- Логи `mlagents-learn`:

![results0](images/results0.png?raw=true)

- Результаты в Tensorboard:

![plots0](images/plots0.png?raw=true)

- Как видно по значениям reward из логов и графиков, система не смогла найти таких параметров, которые бы ограничили рост инфляции

- Можно попробывать замедлить скорость обучения `learning_rate`, чтобы каждый шаг параметры менялись меньше

- Новая конфигурация:

![config1](images/config1.png?raw=true)

- Логи `mlagents-learn` после обучения:

![results1](images/results1.png?raw=true)

- Результаты в Tensorboard:

![plots1](images/plots1.png?raw=true)

- Чуть лучше, но всё равно плохо

- Увеличим значение time_horizon - тем самым, система будет смотреть в будущее на большее число шагов перед тем как обновлять параметры

- Новая конфигурация:

![config2](images/config2.png?raw=true)

- Логи `mlagents-learn` после обучения:

![results2](images/results2.png?raw=true)

- Результаты в Tensorboard:

![plots2](images/plots2.png?raw=true)

- Так как процесс инфляции требует достаточно большого времени(а значит и числа шагов), большее значение time_horizon очень помогло

- Наконец, увеличим гиперпараметр gamma

- Он контролирует на сколько важна награда в будущем

- Новая конфигурация:

![config_final](images/config_final.png?raw=true)

- Логи `mlagents-learn` после обучения:

![results_final](images/results_final.png?raw=true)

- Результаты в Tensorboard:

![plots_final](images/plots_final.png?raw=true)

- После всех этих изменений, система научилась держать уровень инфляции под контролем

## Задание 2

### Пожалуй самый важный график - это Cumulative Reward
- Он показывает, какую в среднем награду агент получает за эпизод
- Если график идёт вверх - агент успешно обучается, вниз - у него проблемы

### Далее идёт Episode Length
- Этот график показывает как быстро в среднем проходит эпизод(одна "жизнь" агента)
- В данном случае - чем короче эпизод, тем хуже. Т.к. при слишком большой инфляции эпизод сразу кончается.

### Policy Loss
- Изменяется в процессе обучения по алгоритму PPO
- Чем выше - тем быстрее меняется внутренний "алгоритм" агента, процесс, по которому он принимает решения

### Value Loss
- Тоже изменяется в процессе обучения по алгоритму PPO
- Чем ниже - тем лучше агент может предсказать сколько награды он может получить с данного момента и до конца эпизода(т.е. на сколько удачно его текущее положение)


## Выводы
Изучена простая экономическая система
Для неё автоматически(при помощи Unity ML-Agents) подобраны параметры, ограничивающие рост инфляции
Изучены подробности алгоритма обучения с подкреплением Proximal Policy Optimization(PPO)

Последнюю версию файла конфигурации обучения можно найти [в этом репозитории](Economic.yaml)
